Abstract A language model based on features extracted from a recurrent neural network language model and semantic embedding of the left context of the current word based on probabilistic semantic analysis (PLSA) is developed. To calculate such embedding, the context is considered as a document. The effect of vanishing gradients in a recurrent neural network is reduced by this method. The experiment has shown that adding topic-based features reduces perplexity by 10%.
recurrent neural network1.0^^http://www.w3.org/2001/XMLSchema#double                          http://dbpedia.org/resource/Recurrent_neural_network

semantic                      0.9995504960497106^^http://www.w3.org/2001/XMLSchema#double           http://dbpedia.org/resource/Semantics             

semantic analysis             0.9953582952376873^^http://www.w3.org/2001/XMLSchema#double           http://dbpedia.org/resource/Compiler              

probabilistic                 0.5608556673642418^^http://www.w3.org/2001/XMLSchema#double           http://dbpedia.org/resource/Statistical_model     

recurrent neural network      1.0^^http://www.w3.org/2001/XMLSchema#double                          http://dbpedia.org/resource/Recurrent_neural_network

language model                1.0^^http://www.w3.org/2001/XMLSchema#double                          http://dbpedia.org/resource/Language_model        

language model                1.0^^http://www.w3.org/2001/XMLSchema#double                          http://dbpedia.org/resource/Language_model        

PLSA                          1.0^^http://www.w3.org/2001/XMLSchema#double                          http://dbpedia.org/resource/Probabilistic_latent_semantic_analysis

gradients                     0.999983420337966^^http://www.w3.org/2001/XMLSchema#double            http://dbpedia.org/resource/Gradient              

