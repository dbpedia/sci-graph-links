Abstract A language model based on features extracted from a recurrent neural network language model and semantic embedding of the left context of the current word based on probabilistic semantic analysis (PLSA) is developed. To calculate such embedding, the context is considered as a document. The effect of vanishing gradients in a recurrent neural network is reduced by this method. The experiment has shown that adding topic-based features reduces perplexity by 10ï¿½prefix=
semantic            0.999950782767416^^http://www.w3.org/2001/XMLSchema#double            http://dbpedia.org/resource/Semantics             

language model                1.0^^http://www.w3.org/2001/XMLSchema#double                          http://dbpedia.org/resource/Language_model        

recurrent neural network      1.0^^http://www.w3.org/2001/XMLSchema#double                          http://dbpedia.org/resource/Recurrent_neural_network

recurrent neural network      1.0^^http://www.w3.org/2001/XMLSchema#double                          http://dbpedia.org/resource/Recurrent_neural_network

gradients                     0.999981035310875^^http://www.w3.org/2001/XMLSchema#double            http://dbpedia.org/resource/Gradient              

semantic analysis             0.9988674961096217^^http://www.w3.org/2001/XMLSchema#double           http://dbpedia.org/resource/Compiler              

prefix                        0.8529828326452736^^http://www.w3.org/2001/XMLSchema#double           http://dbpedia.org/resource/Affix                 

language model                1.0^^http://www.w3.org/2001/XMLSchema#double                          http://dbpedia.org/resource/Language_model        

probabilistic                 0.675335143011768^^http://www.w3.org/2001/XMLSchema#double            http://dbpedia.org/resource/Probability           

PLSA                          1.0^^http://www.w3.org/2001/XMLSchema#double                          http://dbpedia.org/resource/Probabilistic_latent_semantic_analysis

