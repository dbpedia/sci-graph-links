Summary For fixed design regression data kernel estimation is widely used to estimate μ(v)(t), thev-th derivative of μ(t). Denoting such an estimator by $$\hat \mu _{nv} (t),$$ this paper is concerned with the almost sure convergence of $$\hat \mu _{nv} (t) - E\hat \mu _{nv} (t)$$ . It is shown that under several inserting procedures a law of iterated logarithm holds.
nv                  0.7089141917136248^^http://www.w3.org/2001/XMLSchema#double           http://dbpedia.org/resource/Nevada                

regression                    0.9668485614816515^^http://www.w3.org/2001/XMLSchema#double           http://dbpedia.org/resource/Linear_regression     

nv                            0.7089141917136248^^http://www.w3.org/2001/XMLSchema#double           http://dbpedia.org/resource/Nevada                

kernel estimation             1.0^^http://www.w3.org/2001/XMLSchema#double                          http://dbpedia.org/resource/Kernel_(statistics)   

nv                            0.7089141917136248^^http://www.w3.org/2001/XMLSchema#double           http://dbpedia.org/resource/Nevada                

derivative                    0.9999999896786562^^http://www.w3.org/2001/XMLSchema#double           http://dbpedia.org/resource/Derivative            

iterated logarithm            1.0^^http://www.w3.org/2001/XMLSchema#double                          http://dbpedia.org/resource/Iterated_logarithm    

estimator                     0.9674388907836942^^http://www.w3.org/2001/XMLSchema#double           http://dbpedia.org/resource/Estimator             

convergence                   0.9717790028066885^^http://www.w3.org/2001/XMLSchema#double           http://dbpedia.org/resource/Convergence_of_random_variables

